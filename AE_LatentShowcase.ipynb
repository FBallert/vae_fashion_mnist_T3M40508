{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "AE_LatentShowcase",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FBallert/vae_fashion_mnist_T3M40508/blob/main/AE_LatentShowcase.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9H-v9UHYD1R"
      },
      "source": [
        "# Implementation to construct some sample images for a course work paper\r\n",
        "Results may differ slightly because of the probabilistic nature of the neural networks and its parameter initialization.\r\n",
        "\r\n",
        "## Basic imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qB2nU48xMo3f"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehKJSojZYJ5P"
      },
      "source": [
        "# Data set loading\r\n",
        "Loads the Fashion-MNIST (https://research.zalando.com/welcome/mission/research-projects/fashion-mnist/) dataset. Concatenating train and dev set since we are not interested in any supervised learning tasks an assume this as the available amount of unlabelled data instances."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmYqB7K_Mo3i"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\r\n",
        "x_train = np.concatenate((x_train, x_test))\r\n",
        "y_train = np.concatenate((y_train, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjSf8FMsYj9-"
      },
      "source": [
        "Add a dimension to the input since images are requested in the shape of (height, widht, channels) for 2D Convolutional Neural Networks (CNN) in Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eADz-JnRMo3j"
      },
      "source": [
        "x_train = np.expand_dims(x_train, -1).astype(\"float32\") / 255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkRO9hPgahpv"
      },
      "source": [
        "# Classical Autoencoder with convolutional layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJJkI9UtMo3k"
      },
      "source": [
        "inputs = tf.keras.Input(shape=(28, 28, 1))\n",
        "\n",
        "# Encoder\n",
        "x = tf.keras.layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(inputs)\n",
        "x = layers.Flatten()(x)\n",
        "x = tf.keras.layers.Dense(16, activation='relu')(x)\n",
        "z = tf.keras.layers.Dense(2, activation='relu')(x)\n",
        "\n",
        "# Create a dedicated encoder model to get the latent representation for the images later\n",
        "ae_encoder = tf.keras.Model(inputs, z)\n",
        "\n",
        "# Decoder\n",
        "x = tf.keras.layers.Dense(16, activation='relu')(z)\n",
        "x = tf.keras.layers.Dense(14 * 14 * 32, activation='relu')(x)\n",
        "x = tf.keras.layers.Reshape((14, 14, 32))(x)\n",
        "x = tf.keras.layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "outputs = tf.keras.layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
        "\n",
        "# Combine Encoder and Decoder to an autoencoder\n",
        "autoencoder = tf.keras.Model(inputs, outputs, name='classical_conv_autoencoder')\n",
        "\n",
        "autoencoder.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttKqlpQnZFGJ"
      },
      "source": [
        "Extract the decoder from the autoencoder model to later reconstruct images from sampled latent spaces."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jH9etOUSRgTM"
      },
      "source": [
        "# This is our encoded (32-dimensional) input\r\n",
        "latent_input = tf.keras.Input(shape=(2))\r\n",
        "# Retrieve the last layer of the autoencoder model\r\n",
        "decoder_layers = autoencoder.layers[5:10]\r\n",
        "\r\n",
        "# For convenience\r\n",
        "decoded = latent_input\r\n",
        "for layer in decoder_layers:\r\n",
        "  print(layer.name)\r\n",
        "  decoded = layer(decoded)\r\n",
        "# Create the decoder model\r\n",
        "ae_decoder = tf.keras.Model(latent_input, decoded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9YdzgOEZRAL"
      },
      "source": [
        "Train the autoencoder model with Adam. No extensive hyperparameter optimization was done here except of tuning the batch size with respect to the reconstruction loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8cwR7mLMo3m"
      },
      "source": [
        "autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy())\n",
        "\n",
        "history = autoencoder.fit(x_train, x_train,\n",
        "          batch_size=64, epochs=30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YeIBAlVZkCG"
      },
      "source": [
        "Get the latent vectors for all samples in the data set and plot the 2-dimensional latent space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAcI6L7NMo3n"
      },
      "source": [
        "latent_values = ae_encoder.predict(x_train)\r\n",
        "\r\n",
        "font = {'family' : 'sans-serif',\r\n",
        "        'size'   : 24}\r\n",
        "plt.figure(figsize=(15, 13))\r\n",
        "plt.rc('font', **font)\r\n",
        "cmap = plt.get_cmap('RdBu', 10)\r\n",
        "plt.scatter(latent_values[:, 0], latent_values[:, 1], c=y_train, cmap=cmap)\r\n",
        "plt.colorbar()\r\n",
        "plt.xlabel(\"z_dim_0\")\r\n",
        "plt.ylabel(\"z_dim_1\")\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTW4tpl0aYK2"
      },
      "source": [
        "Now we interpolate between two sample data points to see how smoothly the interpolation is going from one sample to another."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlvPSRaXTRQY"
      },
      "source": [
        "# Interpolation between two random samples\r\n",
        "\r\n",
        "random.seed(29)\r\n",
        "random_example_idx = random.randint(0, len(x_train))\r\n",
        "random.seed(30000)\r\n",
        "random_example_2_idx = random.randint(0, len(x_train))\r\n",
        "\r\n",
        "lin_int_space = np.linspace(latent_values[random_example_2_idx], latent_values[random_example_idx], num=10)\r\n",
        "\r\n",
        "def plot_latent_space(n=20, figsize=15):\r\n",
        "    # display a n*n 2D manifold of digits\r\n",
        "    digit_size = 28\r\n",
        "    scale = 1\r\n",
        "    figure = np.zeros((digit_size * n, digit_size * n))\r\n",
        "    # linearly spaced coordinates corresponding to the 2D plot\r\n",
        "    # of digit classes in the latent space\r\n",
        "    grid_x = np.linspace(latent_values[random_example_2_idx], latent_values[random_example_idx], n)\r\n",
        "    grid_y = np.linspace(latent_values[random_example_2_idx], latent_values[random_example_idx], n)[::-1]\r\n",
        "\r\n",
        "    for i, yi in enumerate(grid_y):\r\n",
        "        for j, xi in enumerate(grid_x):\r\n",
        "            z_sample = np.array([xi])\r\n",
        "            x_decoded = ae_decoder.predict(z_sample)\r\n",
        "            digit = x_decoded[0].reshape(digit_size, digit_size)\r\n",
        "            figure[\r\n",
        "                i * digit_size : (i + 1) * digit_size,\r\n",
        "                j * digit_size : (j + 1) * digit_size,\r\n",
        "            ] = digit\r\n",
        "\r\n",
        "    plt.figure(figsize=(figsize, figsize))\r\n",
        "    start_range = digit_size // 2\r\n",
        "    end_range = n * digit_size + start_range\r\n",
        "    pixel_range = np.arange(start_range, end_range, digit_size)\r\n",
        "    sample_range_x = np.round(grid_x, 1)\r\n",
        "    sample_range_y = np.round(grid_y, 1)\r\n",
        "    plt.xticks(pixel_range, sample_range_x)\r\n",
        "    plt.yticks(pixel_range, sample_range_y)\r\n",
        "    plt.xlabel(\"z[0]\")\r\n",
        "    plt.ylabel(\"z[1]\")\r\n",
        "    plt.imshow(figure, cmap=\"Greys_r\")\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "\r\n",
        "plot_latent_space()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijs4EOqNa4dL"
      },
      "source": [
        "# Variational Autoencoder with convolutional layer\r\n",
        "\r\n",
        "The implementation of this VAE is taken from https://keras.io/examples/generative/vae."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4c2iSi4PYg7"
      },
      "source": [
        "class Sampling(layers.Layer):\r\n",
        "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\r\n",
        "\r\n",
        "    def call(self, inputs):\r\n",
        "        z_mean, z_log_var = inputs\r\n",
        "        batch = tf.shape(z_mean)[0]\r\n",
        "        dim = tf.shape(z_mean)[1]\r\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\r\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\r\n",
        "\r\n",
        "latent_dim = 2\r\n",
        "\r\n",
        "encoder_inputs = keras.Input(shape=(28, 28, 1))\r\n",
        "x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\r\n",
        "x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\r\n",
        "x = layers.Flatten()(x)\r\n",
        "x = layers.Dense(16, activation=\"relu\")(x)\r\n",
        "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\r\n",
        "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\r\n",
        "z = Sampling()([z_mean, z_log_var])\r\n",
        "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\r\n",
        "encoder.summary()\r\n",
        "\r\n",
        "latent_inputs = keras.Input(shape=(latent_dim,))\r\n",
        "x = layers.Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\r\n",
        "x = layers.Reshape((7, 7, 64))(x)\r\n",
        "x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\r\n",
        "x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\r\n",
        "decoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\r\n",
        "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\r\n",
        "decoder.summary()\r\n",
        "\r\n",
        "class VAE(keras.Model):\r\n",
        "    def __init__(self, encoder, decoder, **kwargs):\r\n",
        "        super(VAE, self).__init__(**kwargs)\r\n",
        "        self.encoder = encoder\r\n",
        "        self.decoder = decoder\r\n",
        "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\r\n",
        "        self.reconstruction_loss_tracker = keras.metrics.Mean(\r\n",
        "            name=\"reconstruction_loss\"\r\n",
        "        )\r\n",
        "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\r\n",
        "\r\n",
        "    @property\r\n",
        "    def metrics(self):\r\n",
        "        return [\r\n",
        "            self.total_loss_tracker,\r\n",
        "            self.reconstruction_loss_tracker,\r\n",
        "            self.kl_loss_tracker,\r\n",
        "        ]\r\n",
        "\r\n",
        "    def train_step(self, data):\r\n",
        "        with tf.GradientTape() as tape:\r\n",
        "            z_mean, z_log_var, z = self.encoder(data)\r\n",
        "            reconstruction = self.decoder(z)\r\n",
        "            reconstruction_loss = tf.reduce_mean(\r\n",
        "                tf.reduce_sum(\r\n",
        "                    keras.losses.binary_crossentropy(data, reconstruction), axis=(1, 2)\r\n",
        "                )\r\n",
        "            )\r\n",
        "            kl_loss = (-0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)))\r\n",
        "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\r\n",
        "            total_loss = reconstruction_loss + kl_loss\r\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\r\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\r\n",
        "        self.total_loss_tracker.update_state(total_loss)\r\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\r\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\r\n",
        "        return {\r\n",
        "            \"loss\": self.total_loss_tracker.result(),\r\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\r\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\r\n",
        "        }\r\n",
        "\r\n",
        "vae = VAE(encoder, decoder)\r\n",
        "vae.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3))\r\n",
        "vae.fit(x_train, epochs=30, batch_size=256)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhmmxdHjbNAa"
      },
      "source": [
        "Plot the 2-dimensional latent space of the VAE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofZDQIS8diyE"
      },
      "source": [
        "vae_z = vae.encoder.predict(x_train)\r\n",
        "cmap = plt.get_cmap('RdBu', 10)\r\n",
        "plt.figure(figsize=(12, 10))\r\n",
        "plt.scatter(vae_z[0][:, 0], vae_z[0][:, 1], c=y_train, cmap=cmap)\r\n",
        "# plt.scatter(vae_z[0][:, 0], vae_z[0][:, 1], c=y_train, cmap=cmap, alpha=1, s=1)\r\n",
        "plt.colorbar()\r\n",
        "plt.xlabel(\"z_dim_0\")\r\n",
        "plt.ylabel(\"z_dim_1\")\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUgHFG7ybRJ7"
      },
      "source": [
        "Plot the interpolation between the same sample data points as before just for the latent space of the VAE. We see, the transition from the one sample to the other is smoother as in the classical AE case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLos7Pwea4Qj"
      },
      "source": [
        "def plot_latent_space(vae, n=20, figsize=15):\r\n",
        "    # display a n*n 2D manifold of digits\r\n",
        "    digit_size = 28\r\n",
        "    scale = 1\r\n",
        "    figure = np.zeros((digit_size * n, digit_size * n))\r\n",
        "    # linearly spaced coordinates corresponding to the 2D plot\r\n",
        "    # of digit classes in the latent space\r\n",
        "    grid_x = np.linspace(vae_z[0][random_example_2_idx], vae_z[0][random_example_idx], n)\r\n",
        "    grid_y = np.linspace(vae_z[0][random_example_2_idx], vae_z[0][random_example_idx], n)[::-1]\r\n",
        "\r\n",
        "    for i, yi in enumerate(grid_y):\r\n",
        "        for j, xi in enumerate(grid_x):\r\n",
        "            z_sample = np.array([xi])\r\n",
        "            x_decoded = vae.decoder.predict(z_sample)\r\n",
        "            # x_decoded = ae_decoder.predict(z_sample)\r\n",
        "            digit = x_decoded[0].reshape(digit_size, digit_size)\r\n",
        "            figure[\r\n",
        "                i * digit_size : (i + 1) * digit_size,\r\n",
        "                j * digit_size : (j + 1) * digit_size,\r\n",
        "            ] = digit\r\n",
        "\r\n",
        "    plt.figure(figsize=(figsize, figsize))\r\n",
        "    start_range = digit_size // 2\r\n",
        "    end_range = n * digit_size + start_range\r\n",
        "    pixel_range = np.arange(start_range, end_range, digit_size)\r\n",
        "    sample_range_x = np.round(grid_x, 1)\r\n",
        "    sample_range_y = np.round(grid_y, 1)\r\n",
        "    plt.xticks(pixel_range, sample_range_x)\r\n",
        "    plt.yticks(pixel_range, sample_range_y)\r\n",
        "    plt.xlabel(\"z[0]\")\r\n",
        "    plt.ylabel(\"z[1]\")\r\n",
        "    plt.imshow(figure, cmap=\"Greys_r\")\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "\r\n",
        "plot_latent_space(vae)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}